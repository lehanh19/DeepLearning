{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import sklearn\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0, x)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1/m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1/m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0)) # đạo hàm ReLU: g'(x) = x > 0 ? 1 : 0\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of n_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters['W' + str(i)] = ... \n",
    "                  parameters['b' + str(i)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A3, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    loss = - 1/m * np.sum(np.multiply(Y, np.log(A3)) + np.multiply(1 - Y, np.log(1 - A3)))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    A3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Forward propagation\n",
    "    for i in range(0, A3.shape[1]):\n",
    "        if A3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A3>0.5)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    np.random.seed(1)\n",
    "    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n",
    "    np.random.seed(2)\n",
    "    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    test_X = test_X.T\n",
    "    test_Y = test_Y.reshape((1, test_Y.shape[0]))\n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
